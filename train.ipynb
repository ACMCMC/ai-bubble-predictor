{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Bubble Predictor\n",
    "\n",
    "AI is a very trendy topic nowadays. Everyone is talking about it, and we hear about it everywhere. But can it really deliver on its promises?\n",
    "\n",
    "I don't think so. I think that AI is just a bubble that will burst soon. It's a great technology, but it's not as powerful as people think. As any tool, it has its limitations — and those have been overlooked by many.\n",
    "\n",
    "In this project, I will build a model to predict when the AI bubble will burst. I will use historical data about a phenomenon I believe was similar to the AI bubble (the dot-com bubble) to train an LSTM model. Then, I will use this model to generate a hypothesis about when the AI bubble will burst.\n",
    "\n",
    "I hope this project will help people understand the limitations of AI and avoid the consequences of the burst of the AI bubble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get stock market data about the dot-com bubble\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "# Get the stock data\n",
    "full_timeframe_start = datetime.datetime(1996, 1, 1)\n",
    "full_timeframe_end = datetime.datetime(2006, 1, 1)\n",
    "companies = ['MSFT', 'AAPL', 'AMZN', 'INTL', 'ORCL', 'INTC', 'QCOM', 'HPQ', 'EBAY', 'VZ', 'T', 'GOOGL', 'FB', 'TWTR', 'SNAP', 'TSLA', 'NFLX', 'DIS', 'CMCSA', 'FOXA', 'DISCA', 'VIAC', 'NWSA', 'NWS', 'CBS']\n",
    "stock_data = yf.download(tickers=companies, start=full_timeframe_start, end=full_timeframe_end)\n",
    "# Remove those columns that have any NaN values\n",
    "stock_data = stock_data.dropna(axis=1)\n",
    "companies = stock_data.columns.get_level_values(1).unique()\n",
    "num_companies = len(companies)\n",
    "print(f\"Number of companies: {num_companies}\")\n",
    "\n",
    "# Save the data to a CSV file\n",
    "stock_data.to_csv('stock_data.csv')\n",
    "\n",
    "display(stock_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we'll transform the data to make it easier to work with. We'll calculate the daily returns for each stock.\n",
    "# We'll also calculate the average daily return for the dot-com bubble period.\n",
    "# Finally, we'll calculate the average daily return for the period after the dot-com bubble.\n",
    "\n",
    "# Calculate the daily returns\n",
    "stock_data['Adj Close'].pct_change()\n",
    "print(f'Stock data shape: {stock_data.shape}')\n",
    "\n",
    "# Calculate the average daily return for the dot-com bubble period\n",
    "dot_com_bubble_start = full_timeframe_start\n",
    "dot_com_bubble_end = datetime.datetime(2002, 1, 1)\n",
    "\n",
    "dot_com_bubble_returns = stock_data.loc[dot_com_bubble_start:dot_com_bubble_end]['Adj Close'].pct_change().mean()\n",
    "\n",
    "# Calculate the average daily return for the period after the dot-com bubble\n",
    "post_dot_com_bubble_start = dot_com_bubble_end\n",
    "post_dot_com_bubble_end = full_timeframe_end\n",
    "\n",
    "post_dot_com_bubble_returns = stock_data.loc[post_dot_com_bubble_start:post_dot_com_bubble_end]['Adj Close'].pct_change().mean()\n",
    "\n",
    "print('Average daily return for the dot-com bubble period:', dot_com_bubble_returns)\n",
    "print('Average daily return for the period after the dot-com bubble:', post_dot_com_bubble_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's plot the stock prices for each company during the dot-com bubble period.\n",
    "# We'll also plot the average daily return for the dot-com bubble period and the period after the dot-com bubble.\n",
    "\n",
    "# Plot the stock prices for each company during the dot-com bubble period\n",
    "stock_data.loc[dot_com_bubble_start:dot_com_bubble_end]['Adj Close'].plot(figsize=(12, 8))\n",
    "plt.title('Stock Prices During the Dot-Com Bubble Period')\n",
    "plt.legend(companies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, use the transformed data to train an LSTM model to predict the stock prices for the next day.\n",
    "\n",
    "# First, we'll normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(stock_data['Adj Close'])\n",
    "print(f'Scaled data shape: {scaled_data.shape}')\n",
    "\n",
    "import torch\n",
    "# Now, we'll create the input and output sequences for the LSTM model\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        # Generate sequences of length seq_length like: [0, 1, 2, ..., seq_length-1], [1, 2, 3, ..., seq_length], ...\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(data[i+1:i + seq_length + 1])\n",
    "    return torch.FloatTensor(X), torch.FloatTensor(y) # Convert the data to PyTorch tensors\n",
    "\n",
    "seq_length = 60 # i.e. the model will look at the previous 180 days to predict the stock price for the next day\n",
    "\n",
    "X, y = create_sequences(scaled_data, seq_length)\n",
    "\n",
    "# We need to create the sequences before splitting the data into training and testing sets, as the sequences need to be continuous\n",
    "# If we split the data first, the sequences would be broken up and the model wouldn't be able to learn from them\n",
    "# Now, we'll split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False) # This keeps the sequences continuous\n",
    "\n",
    "print(f'Type of X_train: {type(X_train)}')\n",
    "print(f'Type of y_train: {type(y_train)}')\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first element of each sequence in the training set in blue, then plot the first element of each sequence in the testing set in red\n",
    "# This verifies that the sequences are continuous in a visual way\n",
    "plt.plot(range(len(X_train)), X_train[:, 0], label='Training set', color='blue', alpha=0.5)\n",
    "plt.plot(range(len(X_train), len(X_train) + len(X_test)), X_test[:, 0], label='Testing set', color='red', alpha=0.5)\n",
    "plt.title('First Element of Each Sequence in the Training Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we'll build the LSTM model with PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Ensure reproducibility\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=1,\n",
    "        hidden_layer_size=100,\n",
    "        output_size=1,\n",
    "        num_layers=1,\n",
    "        dropout=0.0,\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Apply dropout to the input layer\n",
    "        self.dropout = nn.Dropout(dropout).to(device)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_layer_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=(\n",
    "                dropout if num_layers > 1 else 0\n",
    "            ),  # It doesn't make sense to apply dropout if we only have 1 layer\n",
    "            batch_first=True,\n",
    "        ).to(\n",
    "            device\n",
    "        )  # Input_size is 1 because we're using the daily returns as input (this is just 1 dimension)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size).to(device)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_seq: Variable) -> Variable:\n",
    "        # print(f\"input_seq shape: {input_seq.shape}\")\n",
    "        h0 = torch.zeros(self.num_layers, input_seq.size(0), self.hidden_layer_size).to(\n",
    "            self.device\n",
    "        )\n",
    "        c0 = torch.zeros(self.num_layers, input_seq.size(0), self.hidden_layer_size).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        # Apply dropout to the input layer\n",
    "        input_seq = self.dropout(input_seq)\n",
    "\n",
    "        # We need to detach the hidden state to prevent the model from backpropagating through the entire history\n",
    "        lstm_out, (hn, cn) = self.lstm(input_seq, (h0.detach(), c0.detach()))\n",
    "        # print(f'lstm_out shape: {lstm_out.shape}')\n",
    "\n",
    "        # Apply dropout to the output of the LSTM layer\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "        predictions = self.linear(lstm_out)\n",
    "        # print(f\"predictions shape: {predictions.shape}\")\n",
    "        return predictions\n",
    "\n",
    "    def generate_predictions(\n",
    "        self, input_seq: Variable, num_predictions: int\n",
    "    ) -> Variable:\n",
    "        generated_items = starting_sequence\n",
    "        current_window = starting_sequence\n",
    "\n",
    "        # Prepare the initial hidden state\n",
    "        h0 = torch.zeros(self.num_layers, input_seq.size(0), self.hidden_layer_size).to(\n",
    "            self.device\n",
    "        )\n",
    "        c0 = torch.zeros(self.num_layers, input_seq.size(0), self.hidden_layer_size).to(\n",
    "            self.device\n",
    "        )\n",
    "        for num_item in range(num_to_generate):\n",
    "            lstm_output, (hn, cn) = self.lstm(current_window, (h0, c0))\n",
    "            h0, c0 = hn.detach(), cn.detach()\n",
    "            output = self.linear(lstm_output)\n",
    "            items_to_add_to_window = output[:, -1, :].view(\n",
    "                -1, 1, starting_sequence.shape[-1]\n",
    "            )\n",
    "            current_window = torch.roll(current_window, -1, 1)\n",
    "            current_window[:, -1, :] = items_to_add_to_window\n",
    "            generated_items = torch.cat(\n",
    "                [generated_items, items_to_add_to_window], dim=1\n",
    "            )\n",
    "\n",
    "        return generated_items.view(\n",
    "            starting_sequence.shape[0], -1, starting_sequence.shape[-1]\n",
    "        )\n",
    "\n",
    "    def forward2(self, input_seq: Variable) -> Variable:\n",
    "        # print(f\"input_seq shape: {input_seq.shape}\")\n",
    "        lstm_out, _ = self.lstm(input_seq)\n",
    "        # print(f'lstm_out shape: {lstm_out.shape}')\n",
    "        predictions = self.linear(lstm_out)\n",
    "        # print(f\"predictions shape: {predictions.shape}\")\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If cuda is available, move the model to the GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using CUDA\")\n",
    "    device = torch.device(\"cuda\")\n",
    "# Is mps available?\n",
    "elif torch.backends.mps.is_built():\n",
    "    print(\"Using MPS\")\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "# Only take the first company for now\n",
    "X_train = X_train[:, :, 1:4]\n",
    "y_train = y_train[:, :, 1:4]\n",
    "X_test = X_test[:, :, 1:4]\n",
    "y_test = y_test[:, :, 1:4]\n",
    "\n",
    "# Also move the training data to the GPU\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Device of X_train: {X_train.device}\")\n",
    "print(f\"Type of X_train: {type(X_train)}\")\n",
    "\n",
    "num_layers = 2\n",
    "hidden_layer_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(\n",
    "    input_size=1,\n",
    "    hidden_layer_size=hidden_layer_size,\n",
    "    output_size=1,\n",
    "    num_layers=num_layers,\n",
    "    device=device,\n",
    "    # dropout=0.25,\n",
    ")\n",
    "print(model)\n",
    "\n",
    "loss_function = nn.MSELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "\n",
    "tensor_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "loader = torch.utils.data.DataLoader(tensor_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "epochs = 100\n",
    "log_every = 1\n",
    "import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "INDEX_OF_COMPANY_TO_PLOT = 0\n",
    "\n",
    "\n",
    "# Monitor the training and testing RMSE and stop training when the testing RMSE starts to increase, as this is a sign of overfitting\n",
    "best_test_rmse = float(\"inf\")\n",
    "best_model = None\n",
    "best_epoch = 0\n",
    "patience = 15\n",
    "\n",
    "remaining_patience = patience\n",
    "for epoch in tqdm.tqdm(range(epochs)):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        # X_batch has shape (batch_size, seq_length, num_companies). Turn it into (batch_size, seq_length, input_size)\n",
    "        X_batch_all_companies_together = X_batch.view(-1, seq_length, 1)\n",
    "        # y_batch has shape (batch_size). Turn it into (batch_size, output_size)\n",
    "\n",
    "        y_pred = model(X_batch_all_companies_together)\n",
    "        y_pred = y_pred.view(-1, seq_length, y_batch.shape[-1])\n",
    "        loss = loss_function(y_pred, y_batch)\n",
    "        writer.add_scalar(\"loss\", loss, epoch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    # Monitor the training and testing RMSE\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        num_of_companies_in_batch = X_batch.shape[-1]\n",
    "        # Get the full testing data\n",
    "        X_test_full = X_test\n",
    "        X_test_full = X_test_full.view(-1, seq_length, 1)\n",
    "\n",
    "        y_test_without_prompt = y_test[seq_length:, :, INDEX_OF_COMPANY_TO_PLOT]\n",
    "        starting_sequence = X_test[0, :, INDEX_OF_COMPANY_TO_PLOT].view(\n",
    "            1, -1, 1\n",
    "        )  # First sequence of the first company\n",
    "        num_to_generate = (\n",
    "            X_test.shape[0] - seq_length\n",
    "        )  # We want to generate the rest of the test data, i.e. what's remaining after this first sequence\n",
    "        y_pred = model.generate_predictions(\n",
    "            starting_sequence, num_to_generate\n",
    "        )[:, seq_length:, :]\n",
    "        test_rmse = torch.sqrt(\n",
    "            loss_function(y_pred, y_test_without_prompt)\n",
    "        ).item()\n",
    "    writer.add_scalar(\"test_rmse\", test_rmse, epoch)\n",
    "    print(f\"Epoch {epoch}: test RMSE {test_rmse:.4f}, average loss {total_loss / len(loader):.4f}\")\n",
    "    if test_rmse < best_test_rmse:\n",
    "        best_test_rmse = test_rmse\n",
    "        best_model = model.state_dict()\n",
    "        best_epoch = epoch\n",
    "        print(f\"====> New best!\")\n",
    "\n",
    "    if epoch % log_every == 0:\n",
    "        # Get the full training and testing data for one of the companies (the first one)\n",
    "        y_train_full = y_train[:, -1, INDEX_OF_COMPANY_TO_PLOT]\n",
    "        y_train_full = y_train_full.view(-1, 1)\n",
    "            \n",
    "        # Plot the predictions for the training and testing data. The training data is in blue, the testing data is in red (one goes after the other)\n",
    "        plt.plot(\n",
    "            range(len(y_train_full)),\n",
    "            y_train_full.cpu(),\n",
    "            label=\"Training data\",\n",
    "            color=\"blue\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        plt.plot(\n",
    "            range(len(y_train_full), len(y_train_full) + len(y_test[:,-1, INDEX_OF_COMPANY_TO_PLOT])),\n",
    "            y_test[:, -1, INDEX_OF_COMPANY_TO_PLOT].cpu(),\n",
    "            label=\"Testing data\",\n",
    "            color=\"red\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        plt.plot(\n",
    "            range(len(y_train_full) + seq_length, len(y_train_full) + len(y_test[:,-1, INDEX_OF_COMPANY_TO_PLOT])),\n",
    "            y_pred.squeeze().cpu(),\n",
    "            label=\"Predictions\",\n",
    "            color=\"green\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        plt.title(\"Predictions\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    if test_rmse > best_test_rmse:\n",
    "        remaining_patience -= 1\n",
    "        if remaining_patience == 0:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    else:\n",
    "        remaining_patience = patience\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(best_model)\n",
    "print(f\"Best epoch: {best_epoch}\")\n",
    "\n",
    "writer.close()\n",
    "\n",
    "# Store the model\n",
    "torch.save(model.state_dict(), \"lstm_model.pt\")\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file\n",
    "model = LSTM(\n",
    "    input_size=1,\n",
    "    hidden_layer_size=hidden_layer_size,\n",
    "    output_size=1,\n",
    "    num_layers=num_layers,\n",
    "    device=device,\n",
    ")\n",
    "model.load_state_dict(torch.load(\"lstm_model.pt\"))\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "\n",
    "# Make predictions. We'll get num_companies predictions for each test sequence.\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "# X_test: (num_sequences, seq_length, num_companies)\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test.view(-1, seq_length, 1))\n",
    "    print(f\"y_pred shape: {y_pred.shape}\")\n",
    "\n",
    "# Calculate the loss\n",
    "all_predictions = y_pred\n",
    "print(\"Loss:\", loss.item())\n",
    "# Calculate the average difference between the predicted and actual stock prices\n",
    "average_diff = torch.mean(torch.abs(all_predictions[seq_length:, ...] - y_test[seq_length:, ...]))\n",
    "print(\"Average difference:\", average_diff.item())\n",
    "\n",
    "# Finally, we'll plot the predicted stock prices for the next day.\n",
    "# We'll also plot the actual stock prices for the test data.\n",
    "\n",
    "# Choose a color map so that each company has a different color and the colors are the same for the predicted and actual stock prices\n",
    "cmap = plt.get_cmap(\"tab20\")\n",
    "colors = [cmap(i) for i in range(num_companies)] # These are rbga colors\n",
    "\n",
    "# Plot the percentage change in stock prices for each company\n",
    "# Use the colors defined above to plot the predicted and actual stock prices for each company\n",
    "for i in range(1):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(all_predictions[:, i].cpu().numpy(), label=f\"{companies[i]} (pred)\", linestyle=\"--\", color=colors[i])\n",
    "    plt.plot(y_test[:, i].cpu().numpy(), label=f\"{companies[i]} (actual)\", color=colors[i])\n",
    "    # y range: -1 to 1\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.ylabel(\"Stock Price\")\n",
    "    plt.title(\"Predicted vs. Actual Stock Prices\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform the data\n",
    "y_test_pred = scaler.inverse_transform(all_predictions.cpu().numpy())\n",
    "y_test_actual = scaler.inverse_transform(y_test.cpu().numpy())\n",
    "\n",
    "# Plot the predicted stock prices for the next day\n",
    "for i in range(5):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(y_test_pred[:, i], label=f\"{companies[i]} (pred)\", linestyle=\"--\", color=colors[i])\n",
    "    plt.plot(y_test_actual[:, i], label=f\"{companies[i]} (actual)\", color=colors[i])\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.ylabel(\"Stock Price\")\n",
    "    plt.title(\"Predicted vs. Actual Stock Prices\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_sequence = X_test[0, ...].unsqueeze(dim=0) # First sequence of the first company\n",
    "num_to_generate = X_test.shape[0] - seq_length # We want to generate the rest of the test data, i.e. what's remaining after this first sequence\n",
    "with torch.no_grad():\n",
    "    generations = model.generate_predictions(starting_sequence, num_to_generate)\n",
    "display(generations)\n",
    "\n",
    "# Plot the percentage change in stock prices for each company\n",
    "for i in range(1):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(y_test[:, i].cpu().numpy(), label=f\"{companies[i]} (actual)\", color=colors[i])\n",
    "    # Draw a vertical line after the ending of the prompt (seq_length)\n",
    "    plt.axvline(x=seq_length, color='black', linestyle='--')\n",
    "    plt.plot(range(seq_length, seq_length + num_to_generate), generations[0, seq_length:, i].cpu().numpy(), label=f\"{companies[i]} (pred)\", linestyle=\"--\", color=colors[i])\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.ylabel(\"Stock Price\")\n",
    "    plt.title(\"Predicted vs. Actual Stock Prices\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
